#!/usr/bin/env python3
"""
Comprehensive AIEO Test Suite
Tests all features and capabilities using IT-Journey as example
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "backend"))

from app.services.scoring_engine import ScoringEngine
from app.services.content_parser import ContentParser

# Optional imports (may require database)
try:
    from app.services.benchmark_service import BenchmarkService
    BENCHMARK_AVAILABLE = True
except Exception:
    BENCHMARK_AVAILABLE = False
    BenchmarkService = None

try:
    from app.services.optimize_service import OptimizeService
    OPTIMIZE_AVAILABLE = True
except Exception:
    OPTIMIZE_AVAILABLE = False
    OptimizeService = None


class ComprehensiveTest:
    """Comprehensive test suite for AIEO."""
    
    def __init__(self):
        self.results = {
            "test_run": {
                "timestamp": datetime.now().isoformat(),
                "target": "IT-Journey Repository",
                "version": "0.1.0",
            },
            "tests": {},
            "summary": {},
            "recommendations": [],
        }
        self.scoring_engine = ScoringEngine()
        self.content_parser = ContentParser()
        self.optimize_service = OptimizeService() if OPTIMIZE_AVAILABLE else None
        self.benchmark_service = BenchmarkService() if BENCHMARK_AVAILABLE else None
        
    def run_all_tests(self):
        """Run all test suites."""
        print("=" * 80)
        print("  COMPREHENSIVE AIEO TEST SUITE")
        print("  Target: IT-Journey Repository")
        print("=" * 80)
        print()
        
        # Load test content
        readme_path = Path("/Users/bamr87/github/it-journey/README.md")
        if not readme_path.exists():
            print(f"ERROR: README not found at {readme_path}")
            return
        
        with open(readme_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        print(f"âœ“ Loaded content: {readme_path}")
        print(f"  Length: {len(content):,} characters")
        print(f"  Words: {len(content.split()):,}")
        print()
        
        # Test 1: Content Parsing
        self.test_content_parsing(content)
        
        # Test 2: Scoring Engine
        self.test_scoring_engine(content)
        
        # Test 3: Pattern Detection
        self.test_pattern_detection(content)
        
        # Test 4: Gap Analysis
        self.test_gap_analysis(content)
        
        # Test 5: Benchmark Comparison
        self.test_benchmark(content)
        
        # Test 6: Optimization (if AI service available)
        self.test_optimization(content)
        
        # Test 7: Performance
        self.test_performance(content)
        
        # Generate summary and recommendations
        self.generate_summary()
        self.generate_recommendations()
        
        # Save results
        self.save_results()
        
    def test_content_parsing(self, content: str):
        """Test content parsing capabilities."""
        print("=" * 80)
        print("TEST 1: Content Parsing")
        print("=" * 80)
        
        start_time = time.time()
        parsed = self.content_parser.parse(content, format="markdown")
        duration = time.time() - start_time
        
        test_results = {
            "duration_ms": round(duration * 1000, 2),
            "parsed": {
                "text_length": len(parsed.get("text", "")),
                "headers": len(parsed.get("headers", [])),
                "tables": len(parsed.get("tables", [])),
                "lists": len(parsed.get("lists", [])),
                "code_blocks": len(parsed.get("code_blocks", [])),
                "links": len(parsed.get("links", [])),
                "images": len(parsed.get("images", [])),
            },
            "status": "passed",
        }
        
        print(f"âœ“ Parsing completed in {duration*1000:.2f}ms")
        print(f"  Headers found: {test_results['parsed']['headers']}")
        print(f"  Tables found: {test_results['parsed']['tables']}")
        print(f"  Lists found: {test_results['parsed']['lists']}")
        print(f"  Code blocks found: {test_results['parsed']['code_blocks']}")
        print(f"  Links found: {test_results['parsed']['links']}")
        print()
        
        self.results["tests"]["content_parsing"] = test_results
        
    def test_scoring_engine(self, content: str):
        """Test scoring engine."""
        print("=" * 80)
        print("TEST 2: Scoring Engine")
        print("=" * 80)
        
        start_time = time.time()
        score_result = self.scoring_engine.score(content, format="markdown")
        duration = time.time() - start_time
        
        test_results = {
            "duration_ms": round(duration * 1000, 2),
            "score": score_result.get("score", 0),
            "grade": score_result.get("grade", "F"),
            "word_count": score_result.get("word_count", 0),
            "pattern_scores": score_result.get("pattern_scores", {}),
            "status": "passed",
        }
        
        print(f"âœ“ Scoring completed in {duration*1000:.2f}ms")
        print(f"  Overall Score: {test_results['score']}/100")
        print(f"  Grade: {test_results['grade']}")
        print(f"  Word Count: {test_results['word_count']:,}")
        print()
        print("Pattern Scores:")
        for pattern, data in test_results["pattern_scores"].items():
            score = data.get("score", 0)
            max_score = data.get("max", 0)
            detected = data.get("detected", False)
            status_icon = "âœ“" if detected else "âœ—"
            print(f"  {status_icon} {pattern.replace('_', ' ').title():30s}: {score:5.1f}/{max_score}")
        print()
        
        self.results["tests"]["scoring_engine"] = test_results
        
    def test_pattern_detection(self, content: str):
        """Test individual pattern detection."""
        print("=" * 80)
        print("TEST 3: Pattern Detection")
        print("=" * 80)
        
        patterns = [
            "structured_data",
            "entity_density",
            "citation_hooks",
            "recursive_depth",
            "temporal_anchoring",
            "comparison_tables",
            "definitional_precision",
            "procedural_clarity",
            "faq_injection",
            "meta_context",
        ]
        
        pattern_results = {}
        for pattern in patterns:
            # Check if pattern is detected in score result
            score_result = self.scoring_engine.score(content, format="markdown")
            pattern_data = score_result.get("pattern_scores", {}).get(pattern, {})
            
            pattern_results[pattern] = {
                "detected": pattern_data.get("detected", False),
                "score": pattern_data.get("score", 0),
                "max_score": pattern_data.get("max", 0),
                "percentage": round((pattern_data.get("score", 0) / pattern_data.get("max", 1)) * 100, 1) if pattern_data.get("max", 0) > 0 else 0,
            }
            
            status_icon = "âœ“" if pattern_results[pattern]["detected"] else "âœ—"
            print(f"  {status_icon} {pattern.replace('_', ' ').title():30s}: {pattern_results[pattern]['score']:.1f}/{pattern_results[pattern]['max_score']} ({pattern_results[pattern]['percentage']}%)")
        
        print()
        self.results["tests"]["pattern_detection"] = {
            "patterns": pattern_results,
            "status": "passed",
        }
        
    def test_gap_analysis(self, content: str):
        """Test gap analysis."""
        print("=" * 80)
        print("TEST 4: Gap Analysis")
        print("=" * 80)
        
        score_result = self.scoring_engine.score(content, format="markdown")
        gaps = score_result.get("gaps", [])
        
        # Categorize gaps by severity
        gaps_by_severity = {
            "high": [g for g in gaps if g.get("severity") == "high"],
            "medium": [g for g in gaps if g.get("severity") == "medium"],
            "low": [g for g in gaps if g.get("severity") == "low"],
        }
        
        test_results = {
            "total_gaps": len(gaps),
            "gaps_by_severity": {
                "high": len(gaps_by_severity["high"]),
                "medium": len(gaps_by_severity["medium"]),
                "low": len(gaps_by_severity["low"]),
            },
            "top_gaps": gaps[:10],
            "status": "passed",
        }
        
        print(f"âœ“ Found {len(gaps)} total gaps")
        print(f"  High priority: {test_results['gaps_by_severity']['high']}")
        print(f"  Medium priority: {test_results['gaps_by_severity']['medium']}")
        print(f"  Low priority: {test_results['gaps_by_severity']['low']}")
        print()
        print("Top 5 Gaps:")
        for i, gap in enumerate(gaps[:5], 1):
            print(f"  {i}. [{gap.get('severity', 'unknown').upper()}] {gap.get('category', 'unknown')}")
            print(f"     {gap.get('description', '')}")
        print()
        
        self.results["tests"]["gap_analysis"] = test_results
        
    def test_benchmark(self, content: str):
        """Test benchmark comparison."""
        print("=" * 80)
        print("TEST 5: Benchmark Comparison")
        print("=" * 80)
        
        score_result = self.scoring_engine.score(content, format="markdown")
        score = score_result.get("score", 0)
        
        # Run benchmark (async, but we'll use sync for testing)
        if not self.benchmark_service:
            benchmark = {"percentile": 50, "engine_scores": {}, "error": "Benchmark service not available"}
        else:
            import asyncio
            try:
                benchmark = asyncio.run(self.benchmark_service.calculate_benchmark(content, score))
            except Exception as e:
                benchmark = {"percentile": 50, "engine_scores": {}, "error": str(e)}
        
        test_results = {
            "score": score,
            "percentile": benchmark.get("percentile", 50),
            "engine_scores": benchmark.get("engine_scores", {}),
            "status": "passed",
        }
        
        print(f"âœ“ Benchmark calculated")
        print(f"  Score: {score}/100")
        print(f"  Percentile: {test_results['percentile']}th")
        print(f"  Engine Scores:")
        for engine, engine_score in test_results["engine_scores"].items():
            print(f"    {engine}: {engine_score:.1f}/100")
        print()
        
        self.results["tests"]["benchmark"] = test_results
        
    def test_optimization(self, content: str):
        """Test optimization capabilities."""
        print("=" * 80)
        print("TEST 6: Optimization")
        print("=" * 80)
        
        # Use a sample of content for optimization test
        sample_content = content[:5000]  # First 5000 chars
        
        if not self.optimize_service:
            test_results = {
                "status": "skipped",
                "reason": "Optimization service not available (requires database/AI keys)",
            }
            print(f"âš  Optimization skipped: {test_results['reason']}")
            print()
            self.results["tests"]["optimization"] = test_results
            return
        
        try:
            import asyncio
            start_time = time.time()
            opt_result = asyncio.run(self.optimize_service.optimize(
                content=sample_content,
                style="preserve",
            ))
            duration = time.time() - start_time
            
            test_results = {
                "duration_ms": round(duration * 1000, 2),
                "score_before": opt_result.get("score_before", 0),
                "score_after": opt_result.get("score_after", 0),
                "uplift": opt_result.get("uplift", 0),
                "changes_count": len(opt_result.get("changes", [])),
                "status": "passed",
            }
            
            print(f"âœ“ Optimization completed in {duration*1000:.2f}ms")
            print(f"  Score Before: {test_results['score_before']}/100")
            print(f"  Score After: {test_results['score_after']}/100")
            print(f"  Uplift: +{test_results['uplift']:.1f} points")
            print(f"  Changes Made: {test_results['changes_count']}")
            print()
            
        except Exception as e:
            test_results = {
                "status": "skipped",
                "reason": f"AI service not available: {str(e)}",
            }
            print(f"âš  Optimization skipped: {test_results['reason']}")
            print()
        
        self.results["tests"]["optimization"] = test_results
        
    def test_performance(self, content: str):
        """Test performance metrics."""
        print("=" * 80)
        print("TEST 7: Performance Metrics")
        print("=" * 80)
        
        # Test parsing performance
        parse_times = []
        for _ in range(5):
            start = time.time()
            self.content_parser.parse(content[:10000], format="markdown")
            parse_times.append(time.time() - start)
        
        # Test scoring performance
        score_times = []
        for _ in range(5):
            start = time.time()
            self.scoring_engine.score(content[:10000], format="markdown")
            score_times.append(time.time() - start)
        
        test_results = {
            "parsing": {
                "avg_ms": round(sum(parse_times) / len(parse_times) * 1000, 2),
                "min_ms": round(min(parse_times) * 1000, 2),
                "max_ms": round(max(parse_times) * 1000, 2),
            },
            "scoring": {
                "avg_ms": round(sum(score_times) / len(score_times) * 1000, 2),
                "min_ms": round(min(score_times) * 1000, 2),
                "max_ms": round(max(score_times) * 1000, 2),
            },
            "status": "passed",
        }
        
        print(f"âœ“ Performance metrics calculated")
        print(f"  Parsing: avg {test_results['parsing']['avg_ms']}ms (min: {test_results['parsing']['min_ms']}ms, max: {test_results['parsing']['max_ms']}ms)")
        print(f"  Scoring: avg {test_results['scoring']['avg_ms']}ms (min: {test_results['scoring']['min_ms']}ms, max: {test_results['scoring']['max_ms']}ms)")
        print()
        
        self.results["tests"]["performance"] = test_results
        
    def generate_summary(self):
        """Generate test summary."""
        print("=" * 80)
        print("TEST SUMMARY")
        print("=" * 80)
        
        total_tests = len(self.results["tests"])
        passed_tests = sum(1 for t in self.results["tests"].values() if t.get("status") == "passed")
        skipped_tests = sum(1 for t in self.results["tests"].values() if t.get("status") == "skipped")
        
        overall_score = self.results["tests"].get("scoring_engine", {}).get("score", 0)
        grade = self.results["tests"].get("scoring_engine", {}).get("grade", "F")
        
        self.results["summary"] = {
            "total_tests": total_tests,
            "passed": passed_tests,
            "skipped": skipped_tests,
            "overall_score": overall_score,
            "grade": grade,
            "performance": {
                "parsing_avg_ms": self.results["tests"].get("performance", {}).get("parsing", {}).get("avg_ms", 0),
                "scoring_avg_ms": self.results["tests"].get("performance", {}).get("scoring", {}).get("avg_ms", 0),
            },
        }
        
        print(f"Total Tests: {total_tests}")
        print(f"Passed: {passed_tests}")
        print(f"Skipped: {skipped_tests}")
        print(f"Overall Score: {overall_score}/100 (Grade: {grade})")
        print()
        
    def generate_recommendations(self):
        """Generate improvement recommendations."""
        print("=" * 80)
        print("RECOMMENDATIONS")
        print("=" * 80)
        
        recommendations = []
        
        # Get gap analysis
        gaps = self.results["tests"].get("gap_analysis", {}).get("top_gaps", [])
        pattern_scores = self.results["tests"].get("scoring_engine", {}).get("pattern_scores", {})
        
        # High priority recommendations
        high_gaps = [g for g in gaps if g.get("severity") == "high"]
        for gap in high_gaps[:3]:
            recommendations.append({
                "priority": "high",
                "category": gap.get("category", "unknown"),
                "title": f"Fix {gap.get('category', 'unknown').replace('_', ' ').title()} Gap",
                "description": gap.get("description", ""),
                "expected_impact": "High citation boost (+20-30%)",
                "effort": "Medium",
            })
        
        # Pattern-specific recommendations
        low_scoring_patterns = [
            (p, d) for p, d in pattern_scores.items()
            if d.get("score", 0) / d.get("max", 1) < 0.5
        ]
        
        for pattern, data in low_scoring_patterns[:3]:
            recommendations.append({
                "priority": "medium",
                "category": pattern,
                "title": f"Improve {pattern.replace('_', ' ').title()}",
                "description": f"Current score: {data.get('score', 0)}/{data.get('max', 0)}. Increase {pattern.replace('_', ' ')} to boost citations.",
                "expected_impact": f"Medium citation boost (+10-20%)",
                "effort": "Low",
            })
        
        # Performance recommendations
        perf = self.results["tests"].get("performance", {})
        if perf.get("scoring", {}).get("avg_ms", 0) > 1000:
            recommendations.append({
                "priority": "low",
                "category": "performance",
                "title": "Optimize Scoring Performance",
                "description": f"Scoring takes {perf['scoring']['avg_ms']}ms on average. Consider caching or optimization.",
                "expected_impact": "Better user experience",
                "effort": "High",
            })
        
        self.results["recommendations"] = recommendations
        
        print(f"Generated {len(recommendations)} recommendations:")
        print()
        for i, rec in enumerate(recommendations, 1):
            priority_icon = {"high": "ðŸ”´", "medium": "ðŸŸ¡", "low": "ðŸŸ¢"}.get(rec["priority"], "âšª")
            print(f"{priority_icon} {i}. [{rec['priority'].upper()}] {rec['title']}")
            print(f"   {rec['description']}")
            print(f"   Expected Impact: {rec['expected_impact']}")
            print(f"   Effort: {rec['effort']}")
            print()
        
    def save_results(self):
        """Save test results to file."""
        output_file = Path(__file__).parent.parent.parent / "tools" / "testing" / "comprehensive_results.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print("=" * 80)
        print(f"âœ“ Results saved to: {output_file}")
        print("=" * 80)


if __name__ == "__main__":
    test = ComprehensiveTest()
    test.run_all_tests()

